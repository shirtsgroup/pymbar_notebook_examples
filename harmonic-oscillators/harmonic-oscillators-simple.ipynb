{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does reweighting and MBAR work?\n",
    "\n",
    "This notebook demonstrateds reweighting and MBAR by performing statistical tests on quantities calculated from samples of a set of of 1D harmonic oscillators, for which the true free energy differences can be computed analytically. \n",
    "\n",
    "For a 1D harmonic oscillator with spring constant $K$, the potential is given by\n",
    "\n",
    "$V(x;K) = \\frac{1}{2}K(x-x_0)^2$\n",
    "\n",
    "The equilibrium distribution is given analytically by:\n",
    "\n",
    "$p(x;\\beta,K) = \\sqrt{\\frac{\\beta K}{2\\pi}} e^{-\\frac{\\beta K (x-x_0)^2}{2}}$\n",
    "\n",
    "The dimensionless free energy is therefore:\n",
    "\n",
    "  $f(\\beta,K) = - \\frac{1}{2} \\ln \\left( \\frac{2 \\pi}{\\beta K} \\right)$\n",
    "\n",
    "A number of replications of an experiment in which i.i.d. samples are drawn from a set of $K$ harmonic oscillators are produced.  For each replicate, we estimate the dimensionless free energy differences and mean-square displacements (an observable), as well as their uncertainties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from pymbar import testsystems, exp, exp_gauss, bar, MBAR, FES\n",
    "from pymbar.utils import ParameterError\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up logging for `pymbar` so we can look at the verbose output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing some helper functions we will use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper function for determining how many standard deviations away data is from the expected value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stddev_away(namex, errorx, dx):\n",
    "    if dx > 0:\n",
    "        print(f\"{namex} differs by {errorx / dx:.3f} standard deviations from analytical\")\n",
    "    else:\n",
    "        print(f\"{namex} differs by an undefined number of standard deviations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_analytical`generates the analytical estimates of various functions of a harmonic oscillator.\n",
    "\n",
    "For a harmonic oscillator with spring constant $K$, $x \\sim N(x_0, \\sigma^2$), where $\\sigma = (\\beta K)^{-1/2}$\n",
    "\n",
    "We can calculate expectation values with reweighting as well.  \n",
    "\n",
    "- The dimensionless free energies of each oscillator are $f = - \\ln \\sqrt{\\frac{2 \\pi}{\\beta K}}$, where $K$ is the spring constant. \n",
    "- The expectation of RMS displacement in a harmonic oscillator is just the $\\sigma$ of the oscillator. \n",
    "- The expectation value of the potential energy is $\\frac{1}{2\\beta}$, independent of the spring constant.\n",
    "- The expectation value of the position is simply the mean (center) of the distribution, $x_0$ (or $O$ in the code below)\n",
    "- The expectation value of $\\langle x^2 \\rangle$ is $\\frac{1 + \\beta Kx_0^2}{\\beta K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analytical(beta, K, O, observables):\n",
    "\n",
    "    print(\"Computing dimensionless free energies analytically...\")\n",
    "    sigma = (beta * K) ** -0.5\n",
    "    f_k_analytical = -np.log(np.sqrt(2 * np.pi) * sigma)\n",
    "    Delta_f_ij_analytical = f_k_analytical - np.vstack(f_k_analytical)\n",
    "    A_k_analytical = dict()\n",
    "    A_ij_analytical = dict()\n",
    "    for observe in observables:\n",
    "        if observe == \"RMS displacement\":\n",
    "            # mean square displacement\n",
    "            A_k_analytical[observe] = sigma\n",
    "        if observe == \"potential energy\":\n",
    "            # By equipartition\n",
    "            A_k_analytical[observe] = 1 / (2 * beta) * np.ones(len(K), float)\n",
    "        if observe == \"position\":\n",
    "            # observable is just the position parameter\n",
    "            A_k_analytical[observe] = O \n",
    "        if observe == \"position^2\":\n",
    "            # observable is the position^2\n",
    "            A_k_analytical[observe] = (1 + beta * K * O**2) / (beta * K)\n",
    "        A_ij_analytical[observe] = A_k_analytical[observe] - np.vstack(A_k_analytical[observe])\n",
    "    return f_k_analytical, Delta_f_ij_analytical, A_k_analytical, A_ij_analytical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple, one state reweighting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume because of whatever the issues of our experiments, we can only collect data from a harmonic potential, with harmonic center $\\mu=0$, and $K=1$.  But let's say what we ACTUALLY want, for whatever bizzare reason, are statistics for samples collected with $\\mu=2$ and $K=3$. \n",
    "For simplicity, we set $T=1$, so $\\beta=1$.\n",
    "\n",
    "Here's the potentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_K = 1\n",
    "sampled_mean = 0\n",
    "desired_K = 3\n",
    "desired_mean = 2\n",
    "x=np.linspace(-5,6,1000)\n",
    "plt.plot(x,(sampled_K/2)*(x-sampled_mean)**2,label=\"sampled potential\")\n",
    "plt.plot(x,(desired_K/2)*(x-desired_mean)**2,label=\"desired potential\")\n",
    "plt.ylim([-0.5,20])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the sampled, normalized distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sampled = (2*np.pi/sampled_K)**(-0.5)\n",
    "norm_desired = (2*np.pi/desired_K)**(-0.5)\n",
    "\n",
    "plt.plot(x,norm_sampled*np.exp(-(sampled_K/2)*(x-sampled_mean)**2),label=\"distribution\")\n",
    "plt.plot(x,norm_desired*np.exp(-(x-desired_mean)**2),label=\"desired distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now collect samples from the sampled distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_x = np.random.normal(loc=sampled_mean,scale=sampled_K**(-0.5), size=10000)\n",
    "plt.hist(sampled_x,bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the weights for each SAMPLED point, in the desired distribution, which will be $e^{\\beta \\Delta F- \\beta \\Delta U(x)}$.  \n",
    "\n",
    "What is $\\Delta U$?  $U_{desired}-U_{sampled} = \\frac{1}{2}K_{desired}(x-x_{0,desired})^2 - \\frac{1}{2} K_{sampled}(x-x_{0,sampled})^2$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dU = 0.5 * (desired_K*(sampled_x-desired_mean)**2 - sampled_K*(sampled_x-sampled_mean)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our estimated $\\Delta F = -\\beta^{-1} \\ln \\frac{1}{N}\\sum_{n=1}^N e^{\\beta -\\Delta U(x_n)} = -\\beta^{-1} \\langle e^{\\beta -\\Delta U(x)} \\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dU,bins=30)\n",
    "plt.xlabel(r'$\\Delta U$')\n",
    "plt.ylabel('counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dF = -np.log(np.mean(np.exp(-dU)))\n",
    "print(dF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But a key connetion here - $\\Delta F$ is also the log ratio of the normalizing constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dF = np.log(norm_desired/norm_sampled)\n",
    "print(true_dF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization factor in front of a probability distribution is the partition function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the weights for each of the SAMPLED distribution in the unsampled distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.exp(dF-dU)\n",
    "plt.hist(weights,bins=30)\n",
    "plt.xlabel('weighs')\n",
    "plt.ylabel('counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.exp(dF-dU)\n",
    "plt.plot(sampled_x,weights,ls='',marker='.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the mean of this distribution with a weighted average?  What should it be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sampled_x*weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do multistate reweighting! (MBAR!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Enter some parameters here to describe the MULTIPLE harmonic oscillators we want data for.  For this data, note we have left one state without any samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_k = np.array([25, 16, 9, 4, 1, 1])  # spring constants for each state\n",
    "O_k = np.array([0, 1, 2, 3, 4, 5])  # offsets for spring constants\n",
    "\n",
    "# number of samples from each state (can be zero for some states!)\n",
    "N_k = 10 * np.array([1000, 1000, 1000, 1000, 0, 1000])\n",
    "Nk_ne_zero = N_k != 0\n",
    "beta = 1.0  # inverse temperature for all simulations\n",
    "\n",
    "# Let's define some states we will not collect samples from, but still want to compute properties for.\n",
    "K_extra = np.array([20, 12, 6, 2, 1])  \n",
    "O_extra = np.array([0.5, 1.5, 2.5, 3.5, 4.5])\n",
    "observables = [\"position\", \"position^2\", \"potential energy\", \"RMS displacement\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the harmonic oscillators we are trying to sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = np.linspace(-2,10,10000)\n",
    "for k in range(len(K_k)):\n",
    "    yk = (K_k[k]*(xrange-O_k[k])**2)/2\n",
    "    plt.plot(xrange,yk,label=f\"Potential {k}\") \n",
    "plt.legend()\n",
    "plt.ylim(0,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the probability distributions, to see which ones actually have overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = np.linspace(-2,10,10000)\n",
    "for k in range(len(K_k)):\n",
    "    yk = np.sqrt(K_k[k]/(2*np.pi))*np.exp(-(K_k[k]*(xrange-O_k[k])**2)/2)\n",
    "    plt.plot(xrange,yk,label=f\"Gaussian {k}\") \n",
    "plt.ylim(0,2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the state with center at $x=4$ will have no samples drawn from it.  In many of the examples below, we will be looking at the free energy and expectations of this state using the samples from the OTHER states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random number seeds (allowing for repeatability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = None\n",
    "# Uncomment the following line to seed the random number generated to\n",
    "# produce reproducible output. Seed = 0 is a new random number each time\n",
    "seed = 1234\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the numbers of samples that were specified above for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.size(N_k)\n",
    "if np.shape(K_k) != np.shape(N_k):\n",
    "    msg = f\"K_k ({np.shape(K_k):d}) and N_k ({np.shape(N_k):d}) must have same dimensions.\"\n",
    "    raise ParameterError(msg)\n",
    "if np.shape(O_k) != np.shape(N_k):\n",
    "    msg = f\"O_k ({np.shape(K_k):d}) and N_k ({np.shape(N_k):d}) must have same dimensions.\"\n",
    "    raise ParameterError(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine maximum number of samples drawn for any state, to determine the size of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_max = np.max(N_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_k_analytical, Delta_f_ij_analytical, A_k_analytical, A_ij_analytical = get_analytical(\n",
    "    beta, K_k, O_k, observables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_k_analytical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix of differences between free energies, which is what we actually can compute with MBAR.  This is a square matrix whose entries are $f_i-f_j$, so the diagonals are 0, and it is antisymmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta_f_ij_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This script will draw samples from {K:d} harmonic oscillators.\")\n",
    "print(\"The harmonic oscillators have equilibrium positions:\", O_k)\n",
    "print(\"and spring constants:\", K_k)\n",
    "print(\n",
    "    \"and the following number of samples will be drawn from each\",\n",
    "    \"(can be zero if no samples drawn):\",\n",
    "    N_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, generate independent data samples from $K$ one-dimensional harmonic oscillators centered at the specified locations using some pymbar helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomsample = testsystems.harmonic_oscillators.HarmonicOscillatorsTestCase(\n",
    "    O_k=O_k, K_k=K_k, beta=beta\n",
    ")\n",
    "x_kn, u_kln, N_k = randomsample.sample(N_k, mode=\"u_kln\", seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data we  we actually have generated. We'll look at the distribution of samples as a histogram. Note we have to skip the state we don't generate any samples for, as there is no histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    if N_k[k] > 0:\n",
    "        plt.hist(x_kn[k],bins=30,label=k,alpha=0.5,density=True)\n",
    "    else:\n",
    "        plt.hist(x_kn[k],alpha=0,density=True) # hack to omit the zeros\n",
    "plt.legend()\n",
    "plt.ylim(0,2.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_k$ is the number of samples at each of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(N_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{kn}$ is a 2D array containing the generated $x$ coordinates from each harmonic oscillator, with first index the number of the state, and the second index the index of the sample from that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_kn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$u_{kln}$ is a 3D array, with the first index the state the sample is from, the second index the state at which the energy of this sample is evaluated (the energy is evaluated at all $K$ states, and the 3rd index the number of that sample. \n",
    "\n",
    "There is an alternate way to enter the data (in $u_{kn}$ format) that is useful for data with significantly different numbers of samples per states, and which pymbar actually uses internally, but we'll cover that in a different tutorial.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(u_kln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_kln[:,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested, we can get the un-reduced energies by dividing by $\\beta$ (multiplying by $k_B T$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_kln = u_kln / beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then estimate free energies and expectations of each distribution with MBAR, first initializing it.  To initialize it, we take the $u_{kln}$ matrix, the $N_k$ array of numbers of samples of each state, and various options for nonlinear numerical solutions (we'll use the default solver, and turn on verbose output).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbar = MBAR(u_kln, N_k, relative_tolerance=1.0e-10, verbose=True)\n",
    "# Get matrix of dimensionless free energy differences and uncertainty estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!  We now have all the calculations done to reweight, we just need to plug things in.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the weights.  Remember there are now weights corresponding to what weights the samples would need in each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(mbar.W_nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(x_kn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference between the shape of the weights and the shape of x.   We need to reshape the $x_{kn}$ array by taking only the nonzero states, and flattening this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_x = (x_kn[N_k!=0,:]).flatten()\n",
    "print(flatx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, let's see the weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    plt.plot(flat_x,mbar.W_nk[:,i],marker='.',ls='',label = f'weight in {i}')\n",
    "    plt.xlabel('x-coordinate')\n",
    "    plt.ylabel(f'weights')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `compute_free_energy_differences`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns the free energy differences and the uncertainties using the analytically derived uncertainties in a \"returns\" object.  One can also generate bootstrap uncertainties, but that requires initiallizing MBAR differently, which we'll see later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mbar.compute_free_energy_differences()\n",
    "Delta_f_ij_estimated = results[\"Delta_f\"]\n",
    "dDelta_f_ij_estimated = results[\"dDelta_f\"]\n",
    "print(Delta_f_ij_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the error in our estimated uncertaintes from analytical free energy differences to compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta_f_ij_error = Delta_f_ij_estimated - Delta_f_ij_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error in free energies is:\")\n",
    "print(Delta_f_ij_error)\n",
    "print(\"Uncertainty in free energies is:\")\n",
    "print(dDelta_f_ij_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we have an estimated uncertainty and an actual measured error from the true answer, we can check to see how many standard error intervales our errors are, i.e. is this actually a good estimate of the error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard deviations away is:\")\n",
    "# mathematical manipulation to avoid dividing by zero errors; we don't care\n",
    "# about the diagnonals, since they are identically zero.\n",
    "df_ij_mod = dDelta_f_ij_estimated + np.identity(K)\n",
    "stdevs = np.abs(Delta_f_ij_error / df_ij_mod)\n",
    "for k in range(K):\n",
    "    stdevs[k, k] = 0\n",
    "print(stdevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sample data that the notebook initially comes in, one should see them distributed as one would expect, with most of them under 1 $\\sigma$, and a few bigger (as you would expect). \n",
    "\n",
    "You can actually test the distribution of the errors over a large number of trials under different conditions in the `harmonic-oscillators-quantiles.ipynb notebok`, which has Q-Q plots and other statistics (though this is still being updated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing`bar`\n",
    "\n",
    "Now let's test the implementation of the Bennett acceptance ratio, which should give the same numerical answer to within precision as BAR for the free energy differences between neighboring states (i.e. $f_{i+1}-f_i$). Because it's a 1-dimensional minimization, we can use faster and more robust solvers (specifically the secand method).  But it's a great check. \n",
    "\n",
    "Note that the uncertainty is BAR is a slightly different estimator of uncertainty than MBAR uses.   Generally, the BAR estimator is a little larger than the true error, and MBAR is a little smaller. In the limit of large $N$, the two estimates converge, and they are only different significantly for rather small $N$ (10's per state) if the overlap is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_indices = np.arange(K)[Nk_ne_zero]\n",
    "Knon = len(nonzero_indices)\n",
    "for i in range(Knon - 1):\n",
    "    k = nonzero_indices[i]\n",
    "    k1 = nonzero_indices[i + 1]\n",
    "    w_F = u_kln[k, k1, 0 : N_k[k]] - u_kln[k, k, 0 : N_k[k]]  # forward work\n",
    "    w_R = u_kln[k1, k, 0 : N_k[k1]] - u_kln[k1, k1, 0 : N_k[k1]]  # reverse work\n",
    "    results = bar(w_F, w_R)\n",
    "    df_bar = results[\"Delta_f\"]\n",
    "    ddf_bar = results[\"dDelta_f\"]\n",
    "    bar_analytical = f_k_analytical[k1] - f_k_analytical[k]\n",
    "    bar_error = bar_analytical - df_bar\n",
    "    print(\n",
    "        f\"BAR estimator for reduced free energy from states {k:d} to {k1:d} is {df_bar:f} +/- {ddf_bar:f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"BAR estimator for reduced free energy from states {k:d} to {k1:d} differs from MBAR by: {df_bar-Delta_f_ij_estimated[k,k1]:f}\"\n",
    "    )\n",
    "    stddev_away(\"BAR estimator\", bar_error, ddf_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot calculate BAR to the state with no samples, because it can only be used to calculate the free energies between states with samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `exp`\n",
    "\n",
    "We now test exponential averaging, first in the forward direction.  This is just an automated way of doing the calculation we did manually above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXP forward free energy\")\n",
    "for k in range(K - 1):\n",
    "    if N_k[k] != 0:\n",
    "        # forward work\n",
    "        w_F = u_kln[k, k + 1, 0 : N_k[k]] - u_kln[k, k, 0 : N_k[k]]\n",
    "        results = exp(w_F)\n",
    "        df_exp = results[\"Delta_f\"]\n",
    "        ddf_exp = results[\"dDelta_f\"]\n",
    "        exp_analytical = f_k_analytical[k + 1] - f_k_analytical[k]\n",
    "        exp_error = exp_analytical - df_exp\n",
    "        print(f\"df from states {k:d} to {k + 1:d} is {df_exp:f} +/- {ddf_exp:f}\")\n",
    "        stddev_away(\"df\", exp_error, ddf_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then in the reverse direction for each difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXP reverse free energy\")\n",
    "for k in range(1, K):\n",
    "    if N_k[k] != 0:\n",
    "        w_R = u_kln[k, k - 1, 0 : N_k[k]] - u_kln[k, k, 0 : N_k[k]]  # reverse work\n",
    "        results = exp(w_R)\n",
    "        df_exp = -results[\"Delta_f\"]\n",
    "        ddf_exp = results[\"dDelta_f\"]\n",
    "        exp_analytical = f_k_analytical[k] - f_k_analytical[k - 1]\n",
    "        exp_error = exp_analytical - df_exp\n",
    "        print(f\"df from states {k:d} to {k - 1:d} is {df_exp:f} +/- {ddf_exp:f}\")\n",
    "        stddev_away(\"df\", exp_error, ddf_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing compute_expectations\n",
    "\n",
    "We now compute the expectations of the observables we defined above. To do this, we need to generate an array $A_{kn}$ of each observable, each sample labeled according to the state they were sampled from.  \n",
    "\n",
    "Note there are two modes for observables, one of which is $A_{kn}$ format, with the states labeled, and the other is $A_n$, with the state that each sample is from implicit in the order of the states (the first $N_k[0]$ from state 0, the next $N_k[1]$ from state 1, and so forth for a total of $\\sum_k N_k = N$ total samples.\n",
    "\n",
    "The averages are computed by $\\langle A \\rangle_i = \\sum_{in} w_{ni} x_{n}$, with averages at state $k$ computed with weights corresponding to each sample, which are determined when initiaizing MBAR.  The weight associated with each sample is $w_{in} = \\frac{e^{f_i-u_i(x_n)}}{\\sum_{i=1}^{k} \\frac{N_k}{N}e^{f_k-u_k(x_n)}}$. \n",
    "\n",
    "There are two modes that are checked, one that calculates the averages, and one that calculates the difference between the averages. \n",
    "\n",
    "There is one important flag, which is the state dependence of each sample.  The potential energy is not a single observable but is $K$ different observables, since the potential energy is a different function of the position for each state. In this case, it uses the observable that corresponds to the state that each sample is from.  \n",
    "\n",
    "Note that we do not need to pass the form we are putting the input data in, as the program automatically identifies which one it is depending on whether it's 1D or 2D.\n",
    "\n",
    "We then check the differences between the analytical expectations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_kn_all = dict()\n",
    "A_k_estimated_all = dict()\n",
    "A_kl_estimated_all = dict()\n",
    "N = np.sum(N_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for observe in observables:\n",
    "    print(\"============================================\")\n",
    "    print(f\"      Testing observable '{observe}'\")\n",
    "    print(\"============================================\")\n",
    "    if observe == \"RMS displacement\":\n",
    "        state_dependent = True\n",
    "        A_kn = np.zeros([K, N], dtype=np.float64)\n",
    "        n = 0\n",
    "        for k in range(K):\n",
    "            for nk in range(N_k[k]):\n",
    "                # observable is the squared displacement\n",
    "                A_kn[:, n] = (x_kn[k, nk] - O_k[:]) ** 2\n",
    "                n += 1\n",
    "\n",
    "    # observable is the potential energy, a 3D array since the\n",
    "    # potential energy is a function of the thermodynamic state\n",
    "    elif observe == \"potential energy\":\n",
    "        state_dependent = True\n",
    "        A_kn = np.zeros([K, N], dtype=np.float64)\n",
    "        n = 0\n",
    "        for k in range(0, K):\n",
    "            for nk in range(0, N_k[k]):\n",
    "                A_kn[:, n] = U_kln[k, :, nk]\n",
    "                n += 1\n",
    "\n",
    "    # observable for estimation is the position\n",
    "    elif observe == \"position\":\n",
    "        state_dependent = False\n",
    "        A_kn = np.zeros([K, N_max], dtype=np.float64)\n",
    "        for k in range(0, K):\n",
    "            A_kn[k, 0 : N_k[k]] = x_kn[k, 0 : N_k[k]]\n",
    "\n",
    "    # observable for estimation is the position^2\n",
    "    elif observe == \"position^2\":\n",
    "        state_dependent = False\n",
    "        A_kn = np.zeros([K, N_max], dtype=np.float64)\n",
    "        for k in range(0, K):\n",
    "            A_kn[k, 0 : N_k[k]] = x_kn[k, 0 : N_k[k]] ** 2\n",
    "\n",
    "    results = mbar.compute_expectations(A_kn, state_dependent=state_dependent)\n",
    "    A_k_estimated = results[\"mu\"]\n",
    "    dA_k_estimated = results[\"sigma\"]\n",
    "    # need to additionally transform position**2 to get the square root\n",
    "    if observe == \"RMS displacement\":\n",
    "        A_k_estimated = np.sqrt(A_k_estimated)\n",
    "        # Compute error from analytical observable estimate.\n",
    "        dA_k_estimated = dA_k_estimated / (2 * A_k_estimated)\n",
    "    As_k_estimated = np.zeros([K], np.float64)\n",
    "    dAs_k_estimated = np.zeros([K], np.float64)\n",
    "\n",
    "    # 'standard' expectation averages - not defined if no samples\n",
    "    nonzeros = np.arange(K)[Nk_ne_zero]\n",
    "    totaln = 0\n",
    "    for k in nonzeros:\n",
    "        if (observe == \"position\") or (observe == \"position^2\"):\n",
    "            As_k_estimated[k] = np.average(A_kn[k, 0 : N_k[k]])\n",
    "            dAs_k_estimated[k] = np.sqrt(np.var(A_kn[k, 0 : N_k[k]]) / (N_k[k] - 1))\n",
    "        elif (observe == \"RMS displacement\") or (observe == \"potential energy\"):\n",
    "            totalp = totaln + N_k[k]\n",
    "            As_k_estimated[k] = np.average(A_kn[k, totaln:totalp])\n",
    "            dAs_k_estimated[k] = np.sqrt(np.var(A_kn[k, totaln:totalp]) / (N_k[k] - 1))\n",
    "            totaln = totalp\n",
    "            if observe == \"RMS displacement\":\n",
    "                As_k_estimated[k] = np.sqrt(As_k_estimated[k])\n",
    "                dAs_k_estimated[k] = dAs_k_estimated[k] / (2 * As_k_estimated[k])\n",
    "    A_k_error = A_k_estimated - A_k_analytical[observe]\n",
    "    As_k_error = As_k_estimated - A_k_analytical[observe]\n",
    "    print(\"------------------------------\")\n",
    "    print(\"Now testing 'averages' mode\")\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Analytical estimator of {observe} is\")\n",
    "    print(A_k_analytical[observe])\n",
    "    print(f\"MBAR estimator of the {observe} is\")\n",
    "    print(A_k_estimated)\n",
    "    print(\"MBAR estimators differ by X standard deviations\")\n",
    "    stdevs = np.abs(A_k_error / dA_k_estimated)\n",
    "    print(stdevs)\n",
    "    print(f\"Standard estimator of {observe} is (states with samples):\")\n",
    "    print(As_k_estimated[Nk_ne_zero])\n",
    "    print(\"Standard estimators differ by X standard deviations (states with samples)\")\n",
    "    stdevs = np.abs(As_k_error[Nk_ne_zero] / dAs_k_estimated[Nk_ne_zero])\n",
    "    print(stdevs)\n",
    "    results = mbar.compute_expectations(\n",
    "        A_kn, state_dependent=state_dependent, output=\"differences\"\n",
    "    )\n",
    "    A_kl_estimated = results[\"mu\"]\n",
    "    dA_kl_estimated = results[\"sigma\"]\n",
    "    print(\"------------------------------\")\n",
    "    print(\"Now testing 'differences' mode\")\n",
    "    print(\"------------------------------\")\n",
    "    if (\n",
    "        \"RMS displacement\" != observe\n",
    "    ):  # can't test this, because we're actually computing the expectation of\n",
    "        # the mean square displacement, and so the differences are <a_i^2> - <a_j^2>,\n",
    "        # not sqrt<a_i>^2 - sqrt<a_j>^2\n",
    "        A_kl_analytical = A_k_analytical[observe] - np.vstack(A_k_analytical[observe])\n",
    "        A_kl_error = A_kl_estimated - A_kl_analytical\n",
    "        print(f\"Analytical estimator of differences of {observe} is\")\n",
    "        print(A_kl_analytical)\n",
    "        print(f\"MBAR estimator of the differences of {observe} is\")\n",
    "        print(A_kl_estimated)\n",
    "        print(\"MBAR estimators differ by X standard deviations\")\n",
    "        stdevs = np.abs(A_kl_error / (dA_kl_estimated + np.identity(K)))\n",
    "        for k in range(K):\n",
    "            stdevs[k, k] = 0\n",
    "        print(stdevs)\n",
    "\n",
    "    # We will need these again; so we will save up the A_k for use in compute_multiple_expectations\n",
    "    A_kn_all[observe] = A_kn\n",
    "    A_k_estimated_all[observe] = A_k_estimated\n",
    "    A_kl_estimated_all[observe] = A_kl_estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `compute_multiple_expectations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that computes more than one expectation at the same time. The most important reason to use to use computing_multiple_expectations is when one wants to compute correlations between observables.  We'll skip it here. See `harmonic_oscillators.ipynb` to see its use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `compute_entropy_and_enthalpy`\n",
    "\n",
    "With the energy (or enthaply) and the free energy, we can compute the entropies, including the correlation between the entropy and enthalpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mbar.compute_entropy_and_enthalpy(u_kn=u_kln, verbose=True)\n",
    "Delta_f_ij = results[\"Delta_f\"]\n",
    "dDelta_f_ij = results[\"dDelta_f\"]\n",
    "Delta_u_ij = results[\"Delta_u\"]\n",
    "dDelta_u_ij = results[\"dDelta_u\"]\n",
    "Delta_s_ij = results[\"Delta_s\"]\n",
    "dDelta_s_ij = results[\"dDelta_s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Free energies\")\n",
    "print(Delta_f_ij)\n",
    "print(dDelta_f_ij)\n",
    "diffs1 = Delta_f_ij - Delta_f_ij_estimated\n",
    "print(\n",
    "    f\"maximum difference between values computed here and in computeFreeEnergies is {np.max(diffs1):g}\"\n",
    ")\n",
    "if np.max(np.abs(diffs1)) > 1.0e-10:\n",
    "    print(\"Difference in values from computeFreeEnergies\")\n",
    "    print(diffs1)\n",
    "diffs2 = dDelta_f_ij - dDelta_f_ij_estimated\n",
    "print(\n",
    "    f\"maximum difference between uncertainties computed here and in computeFreeEnergies is {np.max(diffs2):g}\"\n",
    ")\n",
    "if np.max(np.abs(diffs2)) > 1.0e-10:\n",
    "    print(\"Difference in expectations from computeFreeEnergies\")\n",
    "    print(diffs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Energies\")\n",
    "print(Delta_u_ij)\n",
    "print(dDelta_u_ij)\n",
    "U_k = A_k_estimated_all[\"potential energy\"]\n",
    "expectations = U_k - np.vstack(U_k)\n",
    "diffs1 = Delta_u_ij - expectations\n",
    "print(\n",
    "    f\"maximum difference between values computed here and in compute_expectations is {np.max(diffs1):g}\"\n",
    ")\n",
    "if np.max(np.abs(diffs1)) > 1.0e-10:\n",
    "    print(\"Difference in values from compute_expectations\")\n",
    "    print(diffs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entropies\")\n",
    "print(Delta_s_ij)\n",
    "print(dDelta_s_ij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the analytical entropy estimate to allow comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_k_analytical = 0.5 / beta - f_k_analytical\n",
    "Delta_s_ij_analytical = s_k_analytical - np.vstack(s_k_analytical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta_s_ij_error = Delta_s_ij_analytical - Delta_s_ij\n",
    "print(\"Error in entropies is:\")\n",
    "print(Delta_f_ij_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard deviations away is:\")\n",
    "# mathematical manipulation to avoid dividing by zero errors; we don't care\n",
    "# about the diagnonals, since they are identically zero.\n",
    "ds_ij_mod = dDelta_s_ij + np.identity(K)\n",
    "stdevs = np.abs(Delta_s_ij_error / ds_ij_mod)\n",
    "for k in range(K):\n",
    "    stdevs[k, k] = 0\n",
    "print(stdevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `compute_perturbed_free_energies`\n",
    "\n",
    "With this function, we can compute the free energies to states that are not sampled.  This requires inputting an array of energies from unsampled states, of the same form of $u_{kln}$, with the energy of the new state for each sample from each of the $k$ states.  The reason this exists as a separate function is because we don't need to solve any self-consistent equations for states we did NOT sample, so it's very fast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.size(K_extra)\n",
    "f_k_analytical, Delta_f_ij_analytical, A_k_analytical, A_ij_analytical = get_analytical(\n",
    "    beta, K_extra, O_extra, observables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.size(O_extra) != np.size(K_extra):\n",
    "    raise ParameterError(\n",
    "        f\"O_extra ({np.shape(K_k):d}) and K_extra ({np.shape(N_k):d}) must have the same dimensions.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unew_kln = np.zeros([K, L, np.max(N_k)], np.float64)\n",
    "for k in range(K):\n",
    "    for l in range(L):\n",
    "        unew_kln[k, l, 0 : N_k[k]] = (K_extra[l] / 2.0) * (x_kn[k, 0 : N_k[k]] - O_extra[l]) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mbar.compute_perturbed_free_energies(unew_kln)\n",
    "Delta_f_ij_estimated = results[\"Delta_f\"]\n",
    "dDelta_f_ij_estimated = results[\"dDelta_f\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta_f_ij_error = Delta_f_ij_estimated - Delta_f_ij_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error in free energies is:\")\n",
    "print(Delta_f_ij_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard deviations away is:\")\n",
    "# mathematical manipulation to avoid dividing by zero errors; we don't care\n",
    "# about the diagnonals, since they are identically zero.\n",
    "df_ij_mod = dDelta_f_ij_estimated + np.identity(L)\n",
    "stdevs = np.abs(Delta_f_ij_error / df_ij_mod)\n",
    "for l in range(L):\n",
    "    stdevs[l, l] = 0\n",
    "print(stdevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `compute_expectation` (new states)\n",
    "\n",
    "Computing the expectations of different observables at states that are not sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nth = 3\n",
    "# test the nth \"extra\" states, O_extra[nth] & K_extra[nth]\n",
    "for observe in observables:\n",
    "    print(\"============================================\")\n",
    "    print(f\"      Testing observable '{observe}'\")\n",
    "    print(\"============================================\")\n",
    "    if observe == \"RMS displacement\":\n",
    "        state_dependent = True\n",
    "        A_kn = np.zeros([K, 1, N_max], dtype=np.float64)\n",
    "        for k in range(0, K):\n",
    "            # observable is the squared displacement\n",
    "            A_kn[k, 0, 0 : N_k[k]] = (x_kn[k, 0 : N_k[k]] - O_extra[nth]) ** 2\n",
    "\n",
    "    # observable is the potential energy, a 3D array since the potential energy is a function of\n",
    "    # thermodynamic state\n",
    "    elif observe == \"potential energy\":\n",
    "        state_dependent = True\n",
    "        A_kn = unew_kln[:, [nth], :] / beta\n",
    "\n",
    "    # position and position^2 can use the same observables\n",
    "    # observable for estimation is the position\n",
    "    elif observe == \"position\":\n",
    "        state_dependent = False\n",
    "        A_kn = A_kn_all[\"position\"]\n",
    "    elif observe == \"position^2\":\n",
    "        state_dependent = False\n",
    "        A_kn = A_kn_all[\"position^2\"]\n",
    "    A_k_estimated, dA_k_estimated\n",
    "    results = mbar.compute_expectations(\n",
    "        A_kn, unew_kln[:, [nth], :], state_dependent=state_dependent\n",
    "    )\n",
    "    A_k_estimated = results[\"mu\"]\n",
    "    dA_k_estimated = results[\"sigma\"]\n",
    "    # need to additionally transform to get the square root\n",
    "    if observe == \"RMS displacement\":\n",
    "        A_k_estimated = np.sqrt(A_k_estimated)\n",
    "        dA_k_estimated = dA_k_estimated / (2 * A_k_estimated)\n",
    "    A_k_error = A_k_estimated - A_k_analytical[observe][nth]\n",
    "    print(f\"Analytical estimator of {observe} is\")\n",
    "    print(A_k_analytical[observe][nth])\n",
    "    print(f\"MBAR estimator of the {observe} is\")\n",
    "    print(A_k_estimated)\n",
    "    print(\"MBAR estimators differ by X standard deviations\")\n",
    "    stdevs = np.abs(A_k_error / dA_k_estimated)\n",
    "    print(stdevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `compute_overlap`\n",
    "\n",
    "How much overlap do the distributions have? This can be estimated by several overall measures, a matrix of expectations that represents, essentially, the chance that a sample from one state might have actually come from a different state.  \n",
    "\n",
    "Note we cannot compute the overlap from states we did not sample, so the column for 4 is missing here (since we didn't collect any samples).\n",
    "\n",
    "To be able to calculate free energies, we must have a reasonable overlap, which means reasonable values of the off-diagonals, where \"reasonable\" will depend on the number of samples, but generally about 5\\%.  Whwn using the default entries, then the first state is somewhat disconneded from the rest, and thus the uncertainties in these observables are highish. \n",
    "\n",
    "The \"eigenvalues\" reference is the eigenvalues of the matrix, an the scalar measure is $1-\\lambda_1$.  The first eigenvalue of the matrix is 1, but the deviation of the 2nd eigenvalue element from 1 is scalar measure of the total amount of the \"connectedness\" of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mbar.compute_overlap()\n",
    "O = results[\"scalar\"]\n",
    "O_i = results[\"eigenvalues\"]\n",
    "O_ij = results[\"matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overlap matrix output\")\n",
    "with np.printoptions(precision=4, suppress=True):\n",
    "    print(O_ij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just checking that all rows add up to 1, as must be the case for a transition matrix, which this is, because "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(K):\n",
    "    print(f\"Sum of row {k:d} is {np.sum(O_ij[k, :]):f} (should be 1),\", end=\" \")\n",
    "    if np.abs(np.sum(O_ij[k, :]) - 1) < 1.0e-10:\n",
    "        print(\"looks like it is.\")\n",
    "    else:\n",
    "        print(\"but it's not.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eigenvalues of overlap matrix:\")\n",
    "print(O_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start with \\lambda_0, with \\lambda_0 being the first one (which is always 1). \n",
    "print(\"Overlap scalar measure: (1-lambda_1)\")\n",
    "print(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `compute_effective_sample_number`\n",
    "\n",
    "Use the Kish formula ($\\frac{\\sum_n w_n}{\\sum_n w_n^2}$,  or $\\frac{1}{\\sum_n w_n^2}$ if weights are normalized) for estimating how many samples there effectively are.  There are a few different ways to define this, but the Kish estimate is the most common, and has the correct result ($N$) when all weights are equal, or only some fraction of the weights are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_eff = mbar.compute_effective_sample_number(verbose=True)\n",
    "print(\"Effective Sample number\")\n",
    "print(N_eff)\n",
    "print(\"Compare stanadrd estimate of <x> with the MBAR estimate of <x>\")\n",
    "print(\"We should have that with MBAR, err_MBAR = sqrt(N_k/N_eff)*err_standard,\")\n",
    "print(\"so standard (scaled) results should be very close to MBAR results.\")\n",
    "print(\"No standard estimate exists for states that are not sampled.\")\n",
    "A_kn = x_kn\n",
    "results = mbar.compute_expectations(A_kn)\n",
    "val_mbar = results[\"mu\"]\n",
    "err_mbar = results[\"sigma\"]\n",
    "err_standard = np.zeros([K], dtype=np.float64)\n",
    "err_scaled = np.zeros([K], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Testing free energy surface functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equilibrium distribution of a n-d harmonic potential with spring constant $K$ and equilibrium position $\\mu$ is given analytically by:\n",
    "\n",
    "$p(x;\\beta,K) = (\\frac{\\beta K}{2 \\pi})^{d/2} e^{-\\frac{\\beta K(x-\\mu)^2}{2}}$\n",
    "\n",
    "The dimensionless free energy is therefore<br>\n",
    "  $f(\\beta,K) = - \\frac{d}{2} \\ln{\\frac{2 \\pi}{\\beta K}}$\n",
    "\n",
    "In this problem, we are investigating the sum of two Gaussians, one centered at 0, and others centered at each grid point, that are providing umbrella biases to each point. \n",
    "\n",
    "$V(x;K_0) = \\frac{1}{2}K_0(x-x_0)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1-D, The equilibrium distribution of the samples at each umbrella is a product of Gaussians, is given analytically by $p(x;\\beta,K) = \\frac{1}{N} \\exp(-\\frac{\\beta K_0 x^2}{2}  + \\frac{\\beta K_u(x-\\mu)^2}{2})$\n",
    "Where $N$ is the normalization constant associated with this integral.   If we write $K_s = \\frac{K_0 + K_\\mu}{2}$, we get. \n",
    "\n",
    "The dimensionless free energy is the integral of this, and can be computed as:\n",
    "\n",
    "$\\frac{1}{N} \\exp(-\\beta (K_s x^2 - 2 x K_u \\mu  + K_u \\mu^2))$\n",
    "\n",
    "\\[TBD: Do some more stuff with completing the square. The code is right.\\]\n",
    "\n",
    "$f(\\beta,K)           = - \\ln (\\frac{2\\pi}{K_o+K_u})^{d/2} \\exp(-\\frac{K_uK_o \\mu' \\mu}{2(K_o +K_u)})$\n",
    "  \n",
    "$f(\\beta,K) - f_{zero}   = -\\frac{K_u K_o}{2(K_o+K_u)}  = 1/(K_u/2^{-1} + K_0^{-1}/2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to generate data at each of these biased distributions, which supports multiple dimensions, so we can look at 2D FES as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fes_data(\n",
    "    ndim=1, beta = 1, nsamples=1000, K0=20, Ku=100, gridscale=0.2, xrange=((-3,3),)\n",
    "):\n",
    "    x0 = np.zeros([ndim], np.float64)  # center of base potential\n",
    "    numbrellas = 1\n",
    "    nperdim = np.zeros([ndim], int)\n",
    "    for d in range(ndim):\n",
    "        nperdim[d] = xrange[d][1] - xrange[d][0] + 1\n",
    "        numbrellas *= nperdim[d]\n",
    "    print(f\"There are a total of {numbrellas:d} umbrellas.\")\n",
    "\n",
    "    # Enumerate umbrella centers, and compute the analytical free energy of\n",
    "    # that umbrella\n",
    "    print(\"Constructing umbrellas...\")\n",
    "    ksum = (Ku + K0) / beta\n",
    "    kprod = (Ku * K0) / (beta * beta)\n",
    "    f_k_analytical = np.zeros(numbrellas, np.float64)\n",
    "    # xu_i[i,:] is the center of umbrella i\n",
    "    xu_i = np.zeros([numbrellas, ndim], np.float64)\n",
    "    dp = np.zeros(ndim, int)\n",
    "    dp[0] = 1\n",
    "    for d in range(1, ndim):\n",
    "        dp[d] = nperdim[d] * dp[d - 1]\n",
    "    umbrella_zero = 0\n",
    "    for i in range(numbrellas):\n",
    "        center = []\n",
    "        for d in range(ndim):\n",
    "            val = gridscale * ((int(i // dp[d])) % nperdim[d] + xrange[d][0])\n",
    "            center.append(val)\n",
    "        center = np.array(center)\n",
    "        xu_i[i, :] = center\n",
    "        mu2 = np.dot(center, center)\n",
    "        f_k_analytical[i] = np.log(\n",
    "            (ndim * np.pi / ksum) ** (3.0 / 2.0) * np.exp(-kprod * mu2 / (2.0 * ksum))\n",
    "        )\n",
    "        # assumes that we have one state that is at the zero.\n",
    "        if np.all(center == 0.0):\n",
    "            umbrella_zero = i\n",
    "        i += 1\n",
    "        f_k_analytical -= f_k_analytical[umbrella_zero]\n",
    "        print(center)\n",
    "    print(f\"Generating {nsamples:d} samples for each of {numbrellas:d} umbrellas...\")\n",
    "    x_n = np.zeros([numbrellas * nsamples, ndim], np.float64)\n",
    "    for i in range(numbrellas):\n",
    "        for dim in range(ndim):\n",
    "            # Compute mu and sigma for this dimension for sampling from V0(x) + Vu(x).\n",
    "            # Product of Gaussians: N(x ; a, A) N(x ; b, B) = N(a ; b , A+B) x N(x ; c, C) where\n",
    "            # C = 1/(1/A + 1/B)\n",
    "            # c = C(a/A+b/B)\n",
    "            # A = 1/K0, B = 1/Ku\n",
    "            sigma = 1.0 / (K0 + Ku)\n",
    "            mu = sigma * (x0[dim] * K0 + xu_i[i, dim] * Ku)\n",
    "            # Generate normal deviates for this dimension.\n",
    "            x_n[i * nsamples : (i + 1) * nsamples, dim] = np.random.normal(\n",
    "                mu, np.sqrt(sigma), [nsamples]\n",
    "            )\n",
    "    u_kn = np.zeros([numbrellas, nsamples * numbrellas], np.float64)\n",
    "    # Compute reduced potential due to V0.\n",
    "    u_n = beta * (K0 / 2) * np.sum((x_n[:, :] - x0) ** 2, axis=1)\n",
    "    for k in range(numbrellas):\n",
    "        # reduced potential due to umbrella k\n",
    "        uu = beta * (Ku / 2) * np.sum((x_n[:, :] - xu_i[k, :]) ** 2, axis=1)\n",
    "        u_kn[k, :] = u_n + uu\n",
    "    return u_kn, u_n, x_n, f_k_analytical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: 1D free energy profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K0 = 20.0  # the \"unbiased\" state has K_0 = 20.00 (centered at 0). \n",
    "Ku = 100.0  # with the sampled states having KU = 100.00, or 5x steeper. \n",
    "gridscale = 0.2 # The scaling we apply - so the points are gridscale apart.\n",
    "nsamples = 100\n",
    "ndim = 1\n",
    "xrange = [[-3,3]]  # there are xrange[1]-range[0] + 1 umbrella total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot = np.linspace(gridscale*(xrange[0][0]-2),gridscale*(xrange[0][1]+2),1000)\n",
    "u_unbiased = K0*xplot**2\n",
    "plt.plot(xplot, u_unbiased,label='unbiased')\n",
    "for ui in range(0,int(xrange[0][1]-xrange[0][0] + 1)):\n",
    "    center = gridscale*(xrange[0][0] + ui) \n",
    "    plt.plot(xplot,u_unbiased+Ku*(xplot-center)**2,label=f'biased {ui}')\n",
    "plt.ylim(-2,15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = [[-3, 3]]\n",
    "ndim = 1\n",
    "u_kn, u_n, x_n, f_k_analytical = generate_fes_data(\n",
    "    K0=K0,\n",
    "    Ku=Ku,\n",
    "    ndim=ndim,\n",
    "    nsamples=nsamples,\n",
    "    gridscale=gridscale,\n",
    "    xrange=xrange,\n",
    ")\n",
    "numbrellas = (np.shape(u_kn))[0]\n",
    "N_k = nsamples * np.ones([numbrellas], int)\n",
    "print(\"Solving for free energies of state ...\")\n",
    "mbar = MBAR(u_kn, N_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram bins are indexed using the scheme:\n",
    "\n",
    "index = 1 + np.floor((x[0] - xmin)/dx) + nbins*np.floor((x[1] - xmin)/dy)\n",
    "\n",
    "index = 0 is reserved for samples outside of the allowed domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there are 7 states with different free energies. This will be independent of the number of histograms we use to plot any free energy surface.  We'll print them relative to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbar.f_k-np.min(mbar.f_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate a histogram to see how many samples there are, and calculate the free energy at each histogram point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbinsperdim = 15 # the number of bins used to generate the histogram of data. \n",
    "xmin = gridscale * (np.min(xrange[0][0]) - 1 / 2.0)\n",
    "xmax = gridscale * (np.max(xrange[0][1]) + 1 / 2.0)\n",
    "dx = (xmax - xmin) / nbinsperdim\n",
    "nbins = 1 + nbinsperdim**ndim\n",
    "bin_edges = np.linspace(xmin, xmax, nbins)  # list of bin edges.\n",
    "bin_centers = np.zeros([nbins, ndim], np.float64)\n",
    "xplot = 100\n",
    "print(f\"There are {nbins} bins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibin = 1\n",
    "fes_analytical = np.zeros([nbins], np.float64)\n",
    "minmu2 = 1000000\n",
    "zeroindex = 0\n",
    "# construct the bins and the fes\n",
    "for i in range(nbinsperdim):\n",
    "    xbin = xmin + dx * (i + 0.5)\n",
    "    bin_centers[ibin, 0] = xbin\n",
    "    mu2 = xbin * xbin\n",
    "    if mu2 < minmu2:\n",
    "        minmu2 = mu2\n",
    "        zeroindex = ibin\n",
    "    fes_analytical[ibin] = K0 * mu2 / 2.0\n",
    "    ibin += 1\n",
    "fzero = fes_analytical[zeroindex]\n",
    "fes_analytical -= fzero\n",
    "fes_analytical[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_n = np.zeros([numbrellas * nsamples], int)\n",
    "# Determine indices of those within bounds.\n",
    "within_bounds = (x_n[:, 0] >= xmin) & (x_n[:, 0] < xmax)\n",
    "# Determine states for these.\n",
    "bin_n[within_bounds] = 1 + np.floor((x_n[within_bounds, 0] - xmin) / dx)\n",
    "# Determine indices of bins that are not empty.\n",
    "bin_counts = np.zeros([nbins], int)\n",
    "for i in range(nbins):\n",
    "    bin_counts[i] = (bin_n == i).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute free energy profile, first with histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Solving for free energies of state to initialize free energy profile...\")\n",
    "mbar_options = dict()\n",
    "mbar_options[\"verbose\"] = False\n",
    "fes = FES(u_kn, N_k, mbar_options=mbar_options)\n",
    "print(\"Computing free energy profile ...\")\n",
    "histogram_parameters = dict()\n",
    "histogram_parameters[\"bin_edges\"] = bin_edges\n",
    "fes.generate_fes(u_n, x_n, histogram_parameters=histogram_parameters)\n",
    "results = fes.get_fes(\n",
    "    bin_centers[:, 0],\n",
    "    reference_point=\"from-specified\",\n",
    "    fes_reference=0.0,\n",
    "    uncertainty_method=\"analytical\",\n",
    ")\n",
    "f_ih = results[\"f_i\"]\n",
    "df_ih = results[\"df_i\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now estimate the PDF with a kde, using boostrapping to estimate the uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_parameters = dict()\n",
    "kde_parameters[\"bandwidth\"] = dx / 3.0\n",
    "fes.generate_fes(u_n, x_n, fes_type=\"kde\", n_bootstraps=20, kde_parameters=kde_parameters)\n",
    "results_kde = fes.get_fes(\n",
    "    bin_centers,\n",
    "    reference_point=\"from-specified\",\n",
    "    fes_reference=0.0,\n",
    "    uncertainty_method=\"bootstrap\",\n",
    ")\n",
    "f_ik = results_kde[\"f_i\"]\n",
    "df_ik = results_kde[\"df_i\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show free energy and uncertainty of each occupied bin relative to lowest free energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1D free energy profile:\")\n",
    "print(f\"{bin_counts[0]:d} counts out of {numbrellas * nsamples:d} counts not in any bin\")\n",
    "print(\n",
    "    f\"{'bin':>8s} {'x':>6s} {'N':>8s} {'true':>10s}\"\n",
    "    f\"{'f_hist':>10s} {'err_hist':>10s} {'df_hist':>10s} {'sig_hist':>8s}\"\n",
    "    f\"{'f_kde':>10s} {'err_kde':>10s} {'df_kde':>10s} {'sig_kde':>8s}\"\n",
    ")\n",
    "\n",
    "error_h = np.zeros(nbins,float)\n",
    "error_k = np.zeros(nbins,float)\n",
    "\n",
    "for i in range(1, nbins):\n",
    "    error_h[i] = fes_analytical[i] - f_ih[i]\n",
    "    error_k[i] = fes_analytical[i] - f_ik[i]\n",
    "    if df_ih[i] > 0:\n",
    "        stdevs_h = np.abs(error_h[i]) / df_ih[i]\n",
    "    else:\n",
    "        stdevs_h = 0\n",
    "    if df_ik[i] > 0:\n",
    "        stdevs_k = np.abs(error_k[i]) / df_ik[i]\n",
    "    else:\n",
    "        stdevs_k = 0\n",
    "    print(\n",
    "        f\"{i:>8d} {bin_centers[i, 0]:>6.2f} {bin_counts[i]:>8d} {fes_analytical[i]:>10.3f}\"\n",
    "        f\"{f_ih[i]:>10.3f} {error_h[i]:>10.3f} {df_ih[i]:>10.3f} {stdevs_h:>8.2f}\"\n",
    "        f\"{f_ik[i]:>10.3f} {error_k[i]:>10.3f} {df_ik[i]:>10.3f} {stdevs_k:>8.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's graph them the two pdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(bin_centers[1:,], f_ih[1:], yerr=df_ih[1:], label='histogram')\n",
    "plt.errorbar(bin_centers[1:,], f_ik[1:], yerr=df_ik[1:], label='kernel density')\n",
    "plt.plot(bin_centers[1:,],fes_analytical[1:],label='analytical',ls='--')\n",
    "plt.title(\"Comparison of Free Energy Surface estimators\")\n",
    "plt.ylabel(\"free energy surface (units of kT)\")\n",
    "plt.xlabel(\"coordinate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate more data, and plot the results more smoothly. We will use the same bin edges as before, though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = [[-3, 3]]\n",
    "nsamples = 1000\n",
    "ndim = 1\n",
    "u_kn, u_n, x_n, f_k_analytical = generate_fes_data(\n",
    "    K0=K0,\n",
    "    Ku=Ku,\n",
    "    ndim=ndim,\n",
    "    nsamples=nsamples,\n",
    "    gridscale=gridscale,\n",
    "    xrange=xrange,\n",
    ")\n",
    "numbrellas = (np.shape(u_kn))[0]\n",
    "N_k = nsamples * np.ones([numbrellas], int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot = gridscale*np.linspace(xrange[0][0]-0.1,xrange[0][1]+0.1,1000)\n",
    "print(\"Solving for free energies of state to initialize free energy profile...\")\n",
    "mbar_options = dict()\n",
    "mbar_options[\"verbose\"] = False\n",
    "fes = FES(u_kn, N_k, mbar_options=mbar_options, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing free energy profile ...\")\n",
    "kde_parameters = dict()\n",
    "kde_parameters[\"bandwidth\"] = dx / 5.0\n",
    "fes.generate_fes(u_n, x_n, fes_type=\"kde\", kde_parameters=kde_parameters, n_bootstraps=5)\n",
    "results = fes.get_fes(\n",
    "    xplot,\n",
    "    reference_point=\"from-specified\",\n",
    "    fes_reference=0.0,\n",
    "    uncertainty_method=\"bootstrap\",\n",
    ")\n",
    "f_ik = results[\"f_i\"]\n",
    "df_ik = results[\"df_i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(xplot, f_ik, yerr=df_ik, label='kernel density', errorevery=10)\n",
    "plt.plot(bin_centers[1:,],fes_analytical[1:],label='analytical',ls='--')\n",
    "plt.title(\"Comparison of Free Energy Surface estimators\")\n",
    "plt.ylabel(\"free energy surface (units of kT)\")\n",
    "plt.xlabel(\"Torsion angle\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D free energy surface\n",
    "\n",
    "`harmonic_oscillators.ipynb` has an example with a 2D potential of mean force surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
